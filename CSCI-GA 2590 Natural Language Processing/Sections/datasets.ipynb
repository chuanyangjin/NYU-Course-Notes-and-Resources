{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edab610e",
   "metadata": {},
   "source": [
    "## Huggingface Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac4d7c",
   "metadata": {},
   "source": [
    "For this tutorial, you will need to install the following libraries -- datasets, zstandard; This notebook was adopted from the [official tutorial](https://huggingface.co/docs/datasets/tutorial) from Hugginface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b24673ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import psutil\n",
    "from datasets import list_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfdb818",
   "metadata": {},
   "source": [
    "First, let's check all the available datasets in this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cff78b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently 22078 datasets are available on the hub:\n",
      "['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc',\n",
      " 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'air_dialogue',\n",
      " 'ajgt_twitter_ar', 'allegro_reviews', 'allocine', 'alt', 'amazon_polarity',\n",
      " 'amazon_reviews_multi', 'amazon_us_reviews', 'ambig_qa', 'americas_nli', 'ami',\n",
      " 'amttl', 'anli', 'app_reviews', 'aqua_rat', 'aquamuse', 'ar_cov19',\n",
      " 'ar_res_reviews', 'ar_sarcasm', 'arabic_billion_words', 'arabic_pos_dialect',\n",
      " 'arabic_speech_corpus', 'arcd', 'arsentd_lev', 'art', 'arxiv_dataset',\n",
      " 'ascent_kb', 'aslg_pc12', 'asnq', 'asset', 'assin', 'assin2', 'atomic',\n",
      " 'autshumato', 'babi_qa', 'banking77', 'bbaw_egyptian', 'bbc_hindi_nli',\n",
      " 'bc2gm_corpus', 'beans', 'best2009', 'bianet', 'bible_para', 'big_patent',\n",
      " 'billsum', 'bing_coronavirus_query_set', 'biomrc', 'biosses', 'blbooks',\n",
      " 'blbooksgenre', 'blended_skill_talk', 'blimp', 'blog_authorship_corpus',\n",
      " 'bn_hate_speech', 'bnl_newspapers', 'bookcorpus', 'bookcorpusopen', 'boolq',\n",
      " 'bprec', 'break_data', 'brwac', 'bsd_ja_en', 'bswac', 'c3', 'c4', 'cail2018',\n",
      " 'caner', 'capes', 'casino', 'catalonia_independence', 'cats_vs_dogs', 'cawac',\n",
      " 'cbt', 'cc100', 'cc_news', 'ccaligned_multilingual', 'cdsc', 'cdt', 'cedr',\n",
      " 'cfq', 'chr_en', 'cifar10', 'cifar100', 'circa', 'civil_comments',\n",
      " 'clickbait_news_bg', 'climate_fever', 'clinc_oos', 'clue', 'cmrc2018',\n",
      " 'cmu_hinglish_dog', 'cnn_dailymail', 'coached_conv_pref', '21978 more...']\n"
     ]
    }
   ],
   "source": [
    "datasets = list_datasets()\n",
    "\n",
    "print(f\"Currently {len(datasets)} datasets are available on the hub:\")\n",
    "pprint(datasets[:100] + [f\"{len(datasets) - 100} more...\"], compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8e90f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '621ffdd236468d709f181f95',\n",
      " 'author': None,\n",
      " 'cardData': {'annotations_creators': ['crowdsourced'],\n",
      "              'dataset_info': {'config_name': 'plain_text',\n",
      "                               'dataset_size': 89789763,\n",
      "                               'download_size': 35142551,\n",
      "                               'features': [{'dtype': 'string', 'name': 'id'},\n",
      "                                            {'dtype': 'string',\n",
      "                                             'name': 'title'},\n",
      "                                            {'dtype': 'string',\n",
      "                                             'name': 'context'},\n",
      "                                            {'dtype': 'string',\n",
      "                                             'name': 'question'},\n",
      "                                            {'name': 'answers',\n",
      "                                             'sequence': [{'dtype': 'string',\n",
      "                                                           'name': 'text'},\n",
      "                                                          {'dtype': 'int32',\n",
      "                                                           'name': 'answer_start'}]}],\n",
      "                               'splits': [{'name': 'train',\n",
      "                                           'num_bytes': 79317110,\n",
      "                                           'num_examples': 87599},\n",
      "                                          {'name': 'validation',\n",
      "                                           'num_bytes': 10472653,\n",
      "                                           'num_examples': 10570}]},\n",
      "              'language': ['en'],\n",
      "              'language_creators': ['crowdsourced', 'found'],\n",
      "              'license': ['cc-by-4.0'],\n",
      "              'multilinguality': ['monolingual'],\n",
      "              'paperswithcode_id': 'squad',\n",
      "              'pretty_name': 'SQuAD',\n",
      "              'size_categories': ['10K<n<100K'],\n",
      "              'source_datasets': ['extended|wikipedia'],\n",
      "              'task_categories': ['question-answering'],\n",
      "              'task_ids': ['extractive-qa'],\n",
      "              'train-eval-index': [{'col_mapping': {'answers': {'answer_start': 'answer_start',\n",
      "                                                                'text': 'text'},\n",
      "                                                    'context': 'context',\n",
      "                                                    'question': 'question'},\n",
      "                                    'config': 'plain_text',\n",
      "                                    'metrics': [{'name': 'SQuAD',\n",
      "                                                 'type': 'squad'}],\n",
      "                                    'splits': {'eval_split': 'validation',\n",
      "                                               'train_split': 'train'},\n",
      "                                    'task': 'question-answering',\n",
      "                                    'task_id': 'extractive_question_answering'}]},\n",
      " 'citation': '@article{2016arXiv160605250R,\\n'\n",
      "             '       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and '\n",
      "             '{Lopyrev},\\n'\n",
      "             '                 Konstantin and {Liang}, Percy},\\n'\n",
      "             '        title = \"{SQuAD: 100,000+ Questions for Machine '\n",
      "             'Comprehension of Text}\",\\n'\n",
      "             '      journal = {arXiv e-prints},\\n'\n",
      "             '         year = 2016,\\n'\n",
      "             '          eid = {arXiv:1606.05250},\\n'\n",
      "             '        pages = {arXiv:1606.05250},\\n'\n",
      "             'archivePrefix = {arXiv},\\n'\n",
      "             '       eprint = {1606.05250},\\n'\n",
      "             '}',\n",
      " 'description': 'Stanford Question Answering Dataset (SQuAD) is a reading '\n",
      "                'comprehension dataset, consisting of questions posed by '\n",
      "                'crowdworkers on a set of Wikipedia articles, where the answer '\n",
      "                'to every question is a segment of text, or span, from the '\n",
      "                'corresponding reading passage, or the question might be '\n",
      "                'unanswerable.',\n",
      " 'disabled': False,\n",
      " 'downloads': 138799,\n",
      " 'gated': False,\n",
      " 'id': 'squad',\n",
      " 'lastModified': '2022-11-03T16:47:45.000Z',\n",
      " 'likes': 53,\n",
      " 'paperswithcode_id': 'squad',\n",
      " 'private': False,\n",
      " 'sha': '33c0018411a987fa8d219bc1d40adf7dbcc0f920',\n",
      " 'siblings': [],\n",
      " 'tags': ['task_categories:question-answering',\n",
      "          'task_ids:extractive-qa',\n",
      "          'annotations_creators:crowdsourced',\n",
      "          'language_creators:crowdsourced',\n",
      "          'language_creators:found',\n",
      "          'multilinguality:monolingual',\n",
      "          'size_categories:10K<n<100K',\n",
      "          'source_datasets:extended|wikipedia',\n",
      "          'language:en',\n",
      "          'license:cc-by-4.0',\n",
      "          'arxiv:1606.05250']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitishjoshi/miniconda3/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:233: FutureWarning: 'list_datasets' currently returns a list of objects but is planned to be a generator starting from version 0.14 in order to implement pagination. Please avoid to use `list_datasets(...).__len__` or explicitly convert the output to a list first with `list(iter(list_datasets)(...))`.\n",
      "  warnings.warn(self._deprecation_msg.format(attr_name=attr_name), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Check metadata and attributes of a particular dataset\n",
    "\n",
    "squad_dataset = list(list_datasets(with_details=True))[datasets.index('squad')]\n",
    "\n",
    "pprint(squad_dataset.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576292f",
   "metadata": {},
   "source": [
    "Next, let's try loading a dataset and seeing a few examples from the data.\n",
    "\n",
    "The main function we are going to use is `load_dataset` -- this will download the dataset, process and cache it in a structured Arrow table. Arrow table are arbitrarily long tables, typed with types that can be mapped to numpy/pandas/python standard types and can store nested objects. They can be directly access from drive, loaded in RAM or even streamed over the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12ffd898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/Users/nitishjoshi/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "dataset = load_dataset('squad', split='validation[:10%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ff483e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 1057\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Check what the returned dataset looks like\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b70b3170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‰ Dataset len(dataset): 1057\n",
      "\n",
      "ðŸ‘‰ First item 'dataset[0]':\n",
      "{'answers': {'answer_start': [177, 177, 177],\n",
      "             'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']},\n",
      " 'context': 'Super Bowl 50 was an American football game to determine the '\n",
      "            'champion of the National Football League (NFL) for the 2015 '\n",
      "            'season. The American Football Conference (AFC) champion Denver '\n",
      "            'Broncos defeated the National Football Conference (NFC) champion '\n",
      "            'Carolina Panthers 24â€“10 to earn their third Super Bowl title. The '\n",
      "            \"game was played on February 7, 2016, at Levi's Stadium in the San \"\n",
      "            'Francisco Bay Area at Santa Clara, California. As this was the '\n",
      "            '50th Super Bowl, the league emphasized the \"golden anniversary\" '\n",
      "            'with various gold-themed initiatives, as well as temporarily '\n",
      "            'suspending the tradition of naming each Super Bowl game with '\n",
      "            'Roman numerals (under which the game would have been known as '\n",
      "            '\"Super Bowl L\"), so that the logo could prominently feature the '\n",
      "            'Arabic numerals 50.',\n",
      " 'id': '56be4db0acb8001400a502ec',\n",
      " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
      " 'title': 'Super_Bowl_50'}\n"
     ]
    }
   ],
   "source": [
    "# Check size of dataset and see how a particular datapoint looks like\n",
    "\n",
    "print(f\"ðŸ‘‰ Dataset len(dataset): {len(dataset)}\")\n",
    "print(\"\\nðŸ‘‰ First item 'dataset[0]':\")\n",
    "pprint(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8140706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Which NFL team represented the AFC at Super Bowl 50?', 'Which NFL team represented the NFC at Super Bowl 50?', 'Where did Super Bowl 50 take place?', 'Which NFL team won Super Bowl 50?', 'What color was used to emphasize the 50th anniversary of the Super Bowl?', 'What was the theme of Super Bowl 50?', 'What day was the game played on?', 'What is the AFC short for?', 'What was the theme of Super Bowl 50?', 'What does AFC stand for?']\n"
     ]
    }
   ],
   "source": [
    "# You can also get a full column of the dataset by indexing with its name as a string:\n",
    "\n",
    "print(dataset['question'][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ea0e1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:\n",
      "['id', 'title', 'context', 'question', 'answers']\n",
      "Features:\n",
      "{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n",
      " 'context': Value(dtype='string', id=None),\n",
      " 'id': Value(dtype='string', id=None),\n",
      " 'question': Value(dtype='string', id=None),\n",
      " 'title': Value(dtype='string', id=None)}\n",
      "The number of rows 1057 also available as len(dataset) 1057\n",
      "The number of columns 5\n",
      "The shape (rows, columns) (1057, 5)\n"
     ]
    }
   ],
   "source": [
    "# You can also directly access the column names and their types as well as size\n",
    "\n",
    "print(\"Column names:\")\n",
    "pprint(dataset.column_names)\n",
    "print(\"Features:\")\n",
    "pprint(dataset.features)\n",
    "\n",
    "print(\"The number of rows\", dataset.num_rows, \"also available as len(dataset)\", len(dataset))\n",
    "print(\"The number of columns\", dataset.num_columns)\n",
    "print(\"The shape (rows, columns)\", dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba61e48f",
   "metadata": {},
   "source": [
    "**Advantage of the library** --- since the dataset is backed by Apache Arrow Tables, we can load datasets of arbitrary size without worrying about RAM memory limitation (basically the dataset take no space in RAM, it's directly read from drive when needed with fast IO access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57cb83ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 167.78 MB\n"
     ]
    }
   ],
   "source": [
    "# Check RAM usage right now\n",
    "\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79a6d999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6e3092816c4f845b\n",
      "Found cached dataset json (/Users/nitishjoshi/.cache/huggingface/datasets/json/default-6e3092816c4f845b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    }
   ],
   "source": [
    "# Load a large dataset (may take a few minutes)\n",
    "\n",
    "data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\n",
    "pubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f28685a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 600.97 MB\n",
      "Number of files in dataset : 20978892555\n",
      "Dataset size (cache file) : 19.54 GB\n"
     ]
    }
   ],
   "source": [
    "# Check RAM usage now and the size of the dataset\n",
    "\n",
    "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "print(f\"Number of files in dataset : {pubmed_dataset.dataset_size}\")\n",
    "size_gb = pubmed_dataset.dataset_size / (1024**3)\n",
    "print(f\"Dataset size (cache file) : {size_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45cc20",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "The library also provided a useful `map()` function, which is quite useful to preprocess your dataset such as removing stop words, dealing with punctuations or tokenizing the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfe842dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/nitishjoshi/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-b106b7610a1ce672.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My cute title: Super_Bowl_50', 'My cute title: Warsaw']\n"
     ]
    }
   ],
   "source": [
    "# Let's add a prefix 'My cute title: ' to each of our titles\n",
    "\n",
    "def add_prefix_to_title(example):\n",
    "    example['title'] = 'My cute title: ' + example['title']\n",
    "    return example\n",
    "\n",
    "prefixed_dataset = dataset.map(add_prefix_to_title)\n",
    "\n",
    "print(prefixed_dataset.unique('title'))  # `.unique()` is a super fast way to print the unique elemnts in a column (see the doc for all the methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c6e7159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154aeb84a490405a8a83d91088ea018a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1057 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0: Which NFL team represented the AFC at Super Bowl 50?',\n",
      " '1: Which NFL team represented the NFC at Super Bowl 50?',\n",
      " '2: Where did Super Bowl 50 take place?',\n",
      " '3: Which NFL team won Super Bowl 50?',\n",
      " '4: What color was used to emphasize the 50th anniversary of the Super Bowl?']\n"
     ]
    }
   ],
   "source": [
    "# You can also use the indices during map\n",
    "\n",
    "# This will add the index in the dataset to the 'question' field\n",
    "with_indices_dataset = dataset.map(lambda example, idx: {'question': f'{idx}: ' + example['question']},\n",
    "                                   with_indices=True)\n",
    "\n",
    "pprint(with_indices_dataset['question'][:5])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
